# -*- coding: utf-8 -*-
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder
from pygam import LogisticGAM, s, l, te
import os
import warnings
warnings.filterwarnings('ignore')

# æ–°å¢å¯¼å…¥
from scipy.ndimage import gaussian_filter1d

# è®¾ç½®ä¸­æ–‡å­—ä½“æ˜¾ç¤º
plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False  # è§£å†³è´Ÿå·æ˜¾ç¤ºé—®é¢˜

# å…¨å±€å‚æ•°
DATA_CSV = "Q3_person_period.csv"      # è¾“å…¥æ–‡ä»¶ï¼ˆperson-periodæ ¼å¼ï¼‰
OUT_DIR = "output"                     # è¾“å‡ºç›®å½•
V_THRESH = 5                          # éªŒè¯é˜ˆå€¼

# åˆ›å»ºè¾“å‡ºç›®å½•
os.makedirs(OUT_DIR, exist_ok=True)

# å›ºå®šéšæœºç§å­
rng = np.random.default_rng(42)

def normalize_columns(df, cols):
    """å¯¹æŒ‡å®šåˆ—è¿›è¡Œæ ‡å‡†åŒ–"""
    for col in cols:
        if col in df.columns:
            df[col] = (df[col] - df[col].mean()) / df[col].std()
    return df

def unify_keys_and_alias(df):
    """ç»Ÿä¸€åˆ—åå’Œåˆ«å"""
    rename_map = {
        "å­•å¦‡ID": "id",
        "ä½“è´¨æŒ‡æ•°": "BMI", 
        "å‘¨æ•´æ•°": "week",
        "é¦–æ¬¡è¾¾æ ‡å‘¨": "target_week",
        "event": "event"
    }
    
    df = df.rename(columns=rename_map)
    return df

def build_person_period(df, t_start=11, t_end=26):
    """æ„å»ºperson-periodæ ¼å¼æ•°æ®"""
    person_periods = []
    
    for person_id in df['id'].unique():
        person_data = df[df['id'] == person_id].copy()
        
        if person_data['target_week'].isna().all():
            # æœªè¾¾æ ‡æƒ…å†µ
            for t in range(t_start, t_end + 1):
                if t in person_data['week'].values:
                    row = person_data[person_data['week'] == t].iloc[0].copy()
                    row['event'] = 0
                else:
                    row = person_data.iloc[-1].copy()
                    row['week'] = t
                    row['event'] = 0
                person_periods.append(row)
        else:
            # è¾¾æ ‡æƒ…å†µ
            target = person_data['target_week'].iloc[0]
            for t in range(t_start, min(int(target) + 1, t_end + 1)):
                if t in person_data['week'].values:
                    row = person_data[person_data['week'] == t].iloc[0].copy()
                else:
                    row = person_data.iloc[0].copy()
                    row['week'] = t
                
                if t == int(target):
                    row['event'] = 1
                else:
                    row['event'] = 0
                person_periods.append(row)
    
    return pd.DataFrame(person_periods)

# æ–°å¢ï¼šMonte Carlo Fè®¡ç®—å‡½æ•°
def mc_F_for_interval(model, FEATS, df_group, bmi_values, t_values,
                      sigma_week=0.3,    # å­•å‘¨è¯»æ•°æ ‡å‡†å·®ï¼ˆå‘¨ï¼‰
                      se=0.98, sp=0.99,  # æ£€æµ‹çµæ•åº¦/ç‰¹å¼‚åº¦ï¼ˆå¯è°ƒ/æ•æ„Ÿæ€§åˆ†æï¼‰
                      B=200):            # ç»„å†…ZæŠ½æ ·æ¬¡æ•°
    feat_other = [f for f in FEATS if f not in ("week", "BMI")]
    Z_pool = df_group[feat_other].dropna()
    if Z_pool.empty:
        med = {f: float(np.nanmedian(df_group[f])) for f in feat_other}
        Z_draws = pd.DataFrame([med]*B)
    else:
        Z_draws = Z_pool.sample(n=min(B, len(Z_pool)), replace=len(Z_pool)<B,
                                random_state=42).reset_index(drop=True)
    F_out = np.zeros((len(bmi_values), len(t_values)))
    for bi, bmi in enumerate(bmi_values):
        F_acc = np.zeros(len(t_values))
        for _, z in Z_draws.iterrows():
            grid = pd.DataFrame({"week": t_values, "BMI": bmi})
            for k,v in z.items(): 
                grid[k]=v
            h = model.predict_proba(grid[FEATS].to_numpy(float))
            h = se*h + (1-sp)*(1-h)                 # åˆ¤å®šè¯¯å·®æŠ˜ç®—
            if sigma_week>0: 
                h = gaussian_filter1d(h, sigma=sigma_week)  # å­•å‘¨è¯¯å·®
            S = np.cumprod(1 - h + 1e-12)
            F = 1 - S
            F_acc += F
        F_out[bi,:] = F_acc / len(Z_draws)
    return F_out  # å½¢çŠ¶: (len(bmi_values), len(t_values))

# æ–°å¢ï¼šDPåˆ†ç»„æ±‚æœ€ä¼˜æ—¶ç‚¹çš„ä¸‰ä¸ªå‡½æ•°
def interval_risk_and_tstar(F_sub, t_axis, alpha=0.8, c_early=1, c_late=5):
    # å¯è¡Œé›†åˆ: F(t) >= alpha
    feas = np.where(F_sub >= alpha)[0]
    if len(feas)==0: return None, np.inf
    # ç®€åŒ–é£é™©ï¼šP(T>t)=1-F(t)ï¼›P(T<t)â‰ˆF(t-1)
    P_late  = 1 - F_sub[feas]
    P_early = np.r_[0, F_sub[feas][:-1]]  # ä»¥ t-1 çš„Fè¿‘ä¼¼
    risks = c_early*P_late + c_late*P_early
    k = feas[np.argmin(risks)]
    return int(t_axis[k]), float(risks.min())

def precompute_R_for_all_intervals(b_bins, F_mat, t_axis,
                                   nmin_by_bin=None, wmin=2.0, alpha=0.8,
                                   c_early=1, c_late=5):
    N=len(b_bins)
    R=[[None]*N for _ in range(N)]
    Tstar=[[None]*N for _ in range(N)]
    feasible=[[False]*N for _ in range(N)]
    for i in range(N):
        for j in range(i,N):
            if (b_bins[j]-b_bins[i]) < wmin: continue
            if nmin_by_bin is not None:
                nsum = nmin_by_bin[i:j+1].sum()
                if nsum < 30: continue
            F_sub = F_mat[i:j+1,:].mean(axis=0)
            tstar, r = interval_risk_and_tstar(F_sub, t_axis, alpha, c_early, c_late)
            if tstar is not None:
                feasible[i][j]=True
                Tstar[i][j]=tstar
                R[i][j]=r
    return feasible, Tstar, R

def dp_optimal_partition(b_bins, feasible, R, Tstar, lam=0.0):
    N=len(b_bins)
    DP=[np.inf]*N
    PRE=[-1]*N
    for j in range(N):
        best=np.inf
        bi=-1
        for i in range(j+1):
            if not feasible[i][j]: continue
            val=(DP[i-1] if i>0 else 0)+R[i][j]+lam
            if val<best: 
                best=val
                bi=i
        DP[j]=best
        PRE[j]=bi
    cuts=[]
    tstars=[]
    j=N-1
    while j>=0:
        i=PRE[j]
        cuts.append((b_bins[i], b_bins[j]))
        tstars.append(Tstar[i][j])
        j=i-1
    cuts.reverse()
    tstars.reverse()
    return cuts, tstars

def main():
    # 1. æ•°æ®è¯»å–å’Œé¢„å¤„ç†
    print("è¯»å–person-periodæ•°æ®...")
    pp = pd.read_csv(DATA_CSV)
    pp = unify_keys_and_alias(pp)
    
    print(f"Person-periodæ•°æ®å½¢çŠ¶: {pp.shape}")
    print(f"äº‹ä»¶å‘ç”Ÿæ•°: {pp['event'].sum()}")
    
    # 2. ç‰¹å¾å·¥ç¨‹ - æ‰©å±•å¤šå› ç´ ç‰¹å¾
    base_feats = ["week", "BMI"]
    
    # Q3æ–°å¢ï¼šæ‰©å±•å¤šå› ç´ ç‰¹å¾
    extra_feats = [
        "å¹´é¾„","logåŸå§‹è¯»æ®µæ•°","åœ¨å‚è€ƒåŸºå› ç»„ä¸Šæ¯”å¯¹çš„æ¯”ä¾‹",
        "é‡å¤è¯»æ®µçš„æ¯”ä¾‹","GCå«é‡","è¢«è¿‡æ»¤æ‰è¯»æ®µæ•°çš„æ¯”ä¾‹","IVF_æŒ‡ç¤º",
        # Q3æ–°å¢ï¼š
        "èº«é«˜","ä½“é‡","æ£€æµ‹è´¨é‡ä¸»æˆåˆ†1","æ£€æµ‹è´¨é‡ä¸»æˆåˆ†2"
    ]
    FEATS = base_feats + [f for f in extra_feats if f in pp.columns]
    print(f"ä½¿ç”¨çš„ç‰¹å¾: {FEATS}")
    
    # æ•°æ®æ¸…ç†
    pp_clean = pp[FEATS + ['event']].dropna()
    print(f"æ¸…ç†åæ•°æ®å½¢çŠ¶: {pp_clean.shape}")
    
    # 3. GAMæ¨¡å‹è®­ç»ƒ
    print("è®­ç»ƒGAMæ¨¡å‹...")
    
    # æ„å»ºGAM terms - ä¿®å¤æ ·æ¡æ•°é—®é¢˜
    if len(FEATS) == 2:  # åªæœ‰weekå’ŒBMI
        # ç®€åŒ–ç‰ˆæœ¬ï¼Œé¿å…å¤æ‚çš„å¼ é‡é¡¹
        terms = s(0, n_splines=8)  # week
        terms += s(1, n_splines=6)  # BMI
    else:
        # å®Œæ•´ç‰ˆæœ¬
        terms = s(0, n_splines=8)  # week
        terms += s(1, n_splines=6)  # BMI
        terms += te(0, 1, n_splines=[5, 4])  # week*BMIäº¤äº’é¡¹ï¼Œç¡®ä¿éƒ½>3
        
        # æ·»åŠ å…¶ä»–ç‰¹å¾
        for i, feat in enumerate(FEATS[2:], start=2):  # ä»ç¬¬3ä¸ªç‰¹å¾å¼€å§‹
            if pp_clean[feat].nunique() <= 2:  # äºŒå€¼å˜é‡ç”¨çº¿æ€§é¡¹
                terms += l(i)
            else:  # è¿ç»­å˜é‡ç”¨æ ·æ¡
                terms += s(i, n_splines=5)
    
    gam = LogisticGAM(terms)
    X = pp_clean[FEATS].to_numpy(float)
    y = pp_clean['event'].to_numpy()
    
    gam.fit(X, y)
    print(f"GAMæ¨¡å‹è®­ç»ƒå®Œæˆï¼ŒAIC: {gam.statistics_['AIC']:.2f}")
    
    # 4. ç”ŸæˆF(t|BMI)ç½‘æ ¼ - ä½¿ç”¨æ–°çš„MCæ–¹æ³•
    print("ç”ŸæˆF(t|BMI)ç½‘æ ¼...")
    t_seq = np.arange(11, 27)  # 11-26å‘¨
    b_seq = np.arange(18, 46, 0.5)  # BMI 18-45.5ï¼Œæ­¥é•¿0.5
    
    # ğŸŸ¡ æ›¿æ¢ï¼šä½¿ç”¨mc_F_for_intervalæ›¿ä»£æ—§æ–¹æ³•
    F_mat = mc_F_for_interval(
        model=gam, FEATS=FEATS, df_group=pp_clean,
        bmi_values=b_seq, t_values=t_seq,
        sigma_week=0.3, se=0.98, sp=0.99, B=200
    )
    
    # è½¬æ¢ä¸ºDataFrameæ ¼å¼ä¿æŒå…¼å®¹æ€§
    df_grid = (pd.DataFrame(F_mat, index=b_seq, columns=t_seq)
               .stack().rename("F").reset_index()
               .rename(columns={"level_0":"BMI","level_1":"week"}))
    # ä¿æŒå…¼å®¹ï¼šè¡¥S/hå ä½ï¼ˆå›¾ç”¨ F å³å¯ï¼‰
    df_grid = df_grid.sort_values(["BMI","week"])
    df_grid["S"] = 1 - df_grid.groupby("BMI")["F"].transform(lambda x: x)
    df_grid["haz"] = np.nan
    
    print(f"ç½‘æ ¼æ•°æ®å½¢çŠ¶: {df_grid.shape}")
    
    # 5. é‚»åŸŸåˆå¹¶çš„æ”¯æ’‘åº¦è®¡ç®—
    print("è®¡ç®—é‚»åŸŸæ”¯æ’‘åº¦...")
    
    def compute_neighborhood_support(pp_clean, b_seq, t_seq, radius_bmi=1.0, radius_week=1.0):
        """è®¡ç®—é‚»åŸŸåˆå¹¶æ”¯æ’‘åº¦"""
        support_mat = np.zeros((len(b_seq), len(t_seq)))
        
        for i, bmi in enumerate(b_seq):
            for j, week in enumerate(t_seq):
                # å®šä¹‰é‚»åŸŸ
                bmi_mask = (pp_clean['BMI'] >= bmi - radius_bmi) & (pp_clean['BMI'] <= bmi + radius_bmi)
                week_mask = (pp_clean['week'] >= week - radius_week) & (pp_clean['week'] <= week + radius_week)
                support_mat[i, j] = (bmi_mask & week_mask).sum()
        
        return support_mat
    
    support_mat = compute_neighborhood_support(pp_clean, b_seq, t_seq, radius_bmi=1.0, radius_week=1.0)
    
    # 6. DPåˆ†ç»„æ±‚æœ€ä¼˜æ—¶ç‚¹
    print("æ‰§è¡ŒDPåˆ†ç»„ä¼˜åŒ–...")
    
    b_bins = b_seq  # ç›´æ¥ä½¿ç”¨BMIç½‘æ ¼
    feas, Tstar, R = precompute_R_for_all_intervals(
        b_bins=b_bins, F_mat=F_mat, t_axis=t_seq,
        nmin_by_bin=None, wmin=2.0, alpha=0.8,
        c_early=1, c_late=5
    )
    cuts, tstars = dp_optimal_partition(b_bins, feas, R, Tstar, lam=0.0)
    
    # å¯¼å‡ºDPç»“æœ
    dp_results = pd.DataFrame({
        "ç»„åº": range(1, len(cuts)+1),
        "BMIä¸‹ç•Œ": [a for a,b in cuts],
        "BMIä¸Šç•Œ": [b for a,b in cuts],
        "æœ€ä½³æ—¶ç‚¹t*": tstars
    })
    dp_results.to_csv(f"{OUT_DIR}/Q3_DP_åˆ†ç»„ä¸æœ€ä½³æ—¶ç‚¹.csv", index=False, encoding="utf-8-sig")
    print(f"DPç»“æœå·²ä¿å­˜: {len(cuts)}ä¸ªåˆ†ç»„")
    
    # 7. å¯è§†åŒ–
    print("ç”Ÿæˆå›¾è¡¨...")
    
    # 7.1 Fçƒ­å›¾
    plt.figure(figsize=(12, 8))
    F_pivot = df_grid.pivot(index="BMI", columns="week", values="F")
    im = plt.imshow(F_pivot.values, aspect='auto', origin='lower', cmap='viridis',
                   extent=[t_seq.min(), t_seq.max(), b_seq.min(), b_seq.max()])
    plt.colorbar(im, label='F(t|BMI)')
    plt.xlabel('å­•å‘¨ (å‘¨)')
    plt.ylabel('BMI')
    plt.title('Q3: F(t|BMI) çƒ­å›¾ (è¯¯å·®+å¤šå› ç´ MC)')
    plt.tight_layout()
    plt.savefig(f"{OUT_DIR}/Q3_Fçƒ­å›¾.png", dpi=300, bbox_inches='tight')
    plt.close()
    
    # 7.2 é£é™©é›†çƒ­å›¾
    plt.figure(figsize=(12, 8))
    im = plt.imshow(support_mat, aspect='auto', origin='lower', cmap='viridis',
                   extent=[t_seq.min(), t_seq.max(), b_seq.min(), b_seq.max()])
    plt.colorbar(im, label='é‚»åŸŸæ ·æœ¬æ•°')
    plt.xlabel('å­•å‘¨ (å‘¨)')
    plt.ylabel('BMI')
    plt.title('Q3: é‚»åŸŸåˆå¹¶æ”¯æ’‘åº¦çƒ­å›¾')
    plt.tight_layout()
    plt.savefig(f"{OUT_DIR}/Q3_é£é™©é›†çƒ­å›¾.png", dpi=300, bbox_inches='tight')
    plt.close()
    
    # 7.3 æ©è†œçƒ­å›¾
    plt.figure(figsize=(12, 8))
    mask = support_mat >= 30  # æ”¯æ’‘åº¦é˜ˆå€¼
    F_masked = F_mat.copy()
    F_masked[~mask] = np.nan
    
    im = plt.imshow(F_masked, aspect='auto', origin='lower', cmap='viridis',
                   extent=[t_seq.min(), t_seq.max(), b_seq.min(), b_seq.max()])
    plt.colorbar(im, label='F(t|BMI)')
    plt.xlabel('å­•å‘¨ (å‘¨)')
    plt.ylabel('BMI')
    plt.title('Q3: F(t|BMI) çƒ­å›¾ (æ”¯æ’‘åº¦æ©è†œå)')
    plt.tight_layout()
    plt.savefig(f"{OUT_DIR}/Q3_Fçƒ­å›¾_masked.png", dpi=300, bbox_inches='tight')
    plt.close()
    
    # 8. å¯¼å‡ºæ•°æ®
    print("å¯¼å‡ºæ•°æ®...")
    
    # Fç½‘æ ¼æ•°æ®
    with pd.ExcelWriter(f"{OUT_DIR}/Q3_F_grid.xlsx", engine='openpyxl') as writer:
        df_grid.to_excel(writer, sheet_name='F_grid', index=False)
        # æ·»åŠ æ”¯æ’‘åº¦æ•°æ®
        support_df = pd.DataFrame(support_mat, index=b_seq, columns=t_seq)
        support_df.index.name = 'BMI'
        support_df.columns.name = 'week'
        support_df.reset_index().to_excel(writer, sheet_name='support', index=False)
    
    print("Q3åˆ†æå®Œæˆ!")
    print(f"è¾“å‡ºæ–‡ä»¶:")
    print(f"- {OUT_DIR}/Q3_F_grid.xlsx")
    print(f"- {OUT_DIR}/Q3_Fçƒ­å›¾.png")
    print(f"- {OUT_DIR}/Q3_é£é™©é›†çƒ­å›¾.png")
    print(f"- {OUT_DIR}/Q3_Fçƒ­å›¾_masked.png")
    print(f"- {OUT_DIR}/Q3_DP_åˆ†ç»„ä¸æœ€ä½³æ—¶ç‚¹.csv")

if __name__ == "__main__":
    main()